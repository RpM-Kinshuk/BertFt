# BertFt

This research project explores different methods of fine-tuning the pre-trained Google BERT model on various datasets from the General Language Understanding Evaluation (GLUE) benchmark.
<br>
The project also highlights the variations in performance observed over different training layers of the model.
<br> <br>
The results obtained thus far are new to the academia and have not been obtained before.

<h3> CoLA </h2>

![small_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/4d7baaa4-dd42-4f74-bcb2-fa57c33c3085)
![large_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/ac2b0a89-5084-45b0-99b3-2763f00d81b7)

<h3> MRPC </h3>

![small_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/6bccb453-f650-411e-b13e-732fd8514043)
![large_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/f6237f00-d0b3-4f98-862b-d0dcb3eca87e)


<h3> QNLI </h3>

![small_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/ee68d381-93af-4784-ad52-082561d669d3)
![large_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/b91204dc-1f8c-4961-bece-45c00e8b090b)


<h3> RTE </h3>

![small_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/f217c0b2-4a44-4578-8de5-3a72398cca61)
![large_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/46cfa5bf-685f-4fea-af3a-d5e6bb2feea4)


<h3> SST-2 </h3>

![small_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/a3f9d4ac-6b84-48bd-a9b3-511c29af3d6b)
![large_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/de913a51-bd4e-46ae-8293-13f12b3d64c5)

<h3> STSB </h3>

![small_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/626a1e79-d680-451e-a1d3-976a7d18c61e)
![large_alph](https://github.com/RpM-Kinshuk/BertFt/assets/103813028/19d79ae0-7d7a-4752-9def-8a2d75333997)
